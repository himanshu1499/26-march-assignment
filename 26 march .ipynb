{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429ac030-4b7d-4c08-9403-3a000fd00c27",
   "metadata": {},
   "source": [
    "# Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1589a71-3ef9-49c3-9225-88dea372b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression \n",
    "Simple linear regression involves only one independent variable and one dependent variable.\n",
    "The goal is to find the linear relationship between the two variables.\n",
    "It assumes that the relationship between the dependent variable and the independent variable is linear, meaning that the change in the dependent variable is proportional to the change in the independent variable.\n",
    "\n",
    "# example \n",
    "A study to determine the relationship between the hours of studying per week and the GPA of a student.\n",
    "The independent variable would be the number of hours studied per week, and the dependent variable would be the student's GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3c73c-8791-4fce-8655-4311406571d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple linear \n",
    "Multiple linear regression involves two or more independent variables and one dependent variable. \n",
    "The goal is to find the linear relationship between the dependent variable and all the independent variables.\n",
    "It assumes that the relationship between the dependent variable and each independent variable is linear, and that the independent variables are not highly correlated with each other.\n",
    "\n",
    "# example\n",
    "determine the relationship between the price of a house and its various characteristics, such as the number of bedrooms, the square footage, and the location.\n",
    "The independent variables would be the number of bedrooms, the square footage, and the location, and the dependent variable would be the price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ed546-de03-4163-a42a-63e753cc40d3",
   "metadata": {},
   "source": [
    "# Q2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d646fa3-4699-4310-abd4-2355e0b8c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the assumptions of linear regression\n",
    "1.Linearity: There should be a linear relationship between the dependent variable and the independent variables.\n",
    "A scatter plot can be used to check this assumption.\n",
    "\n",
    "2.Homoscedasticity: The variance of the residuals should be constant across all values of the independent variables.\n",
    "This assumption can be checked using a scatter plot of residuals against the predicted values.\n",
    "\n",
    "3.Independence: The observations in the data set should be independent of each other.\n",
    "This assumption can be checked using a scatter plot of residuals against the order of the observations.\n",
    "\n",
    "4.Normality: The residuals should be normally distributed.\n",
    "This assumption can be checked using a histogram or a normal probability plot of the residuals.\n",
    "\n",
    "5.No multicollinearity: The independent variables should not be highly correlated with each other. \n",
    "This assumption can be checked using a correlation matrix or a variance inflation factor.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, several diagnostic plots can be used. \n",
    "Scatter plots can be used to check the linearity assumption, and scatter plots of residuals against the predicted values or order of the observations can be used to check the homoscedasticity and independence assumptions, respectively. \n",
    "Histograms or normal probability plots of the residuals can be used to check the normality assumption.\n",
    "A correlation matrix or VIF can be used to check for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c02ea-a658-4c11-9b63-fc63850ad6c2",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc1776-1a00-4741-afde-9d95856b5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope\n",
    "The slope represents the change in the dependent variable associated with a one-unit change in the independent variable.\n",
    "If the slope is positive, it indicates that the dependent variable increases as the independent variable increases.\n",
    "If the slope is negative, it indicates that the dependent variable decreases as the independent variable increases.\n",
    "The magnitude of the slope indicates the strength of the relationship between the dependent variable and the independent variable\n",
    "\n",
    "# intercept\n",
    "The intercept represents the value of the dependent variable when all the independent variables are equal to zero.\n",
    "It may or may not have a meaningful interpretation depending on the context of the problem.\n",
    "\n",
    "# example\n",
    "we want to model the relationship between the number of hours of exercise per week and the weight loss of a person.\n",
    "We collect data from 100 individuals and fit a linear regression model.\n",
    "The results show that the intercept is 5.5 and the slope is -0.8. \n",
    "This means that if an individual does not exercise at all, their expected weight loss is 5.5 pounds. \n",
    "For every additional hour of exercise per week, the expected weight loss decreases by 0.8 pounds.\n",
    "\n",
    "Therefore, if an individual exercises 5 hours per week, their expected weight loss is 1.5 pounds, calculated as follows:\n",
    "\n",
    "Expected weight loss = intercept + slope * number of hours of exercise per week\n",
    "Expected weight loss = 5.5 + (-0.8) * 5\n",
    "Expected weight loss = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a0c15-d7e0-4c4d-b8b7-7dd9b076b8ba",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a8493-718f-45db-bbd9-de1f827eac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent is an optimization algorithm used to find the minimum value of a function.\n",
    "In machine learning, it is used to minimize the cost function of a model by adjusting the model parameters.\n",
    "The idea behind gradient descent is to iteratively adjust the model parameters in the direction of the steepest descent of the cost function. \n",
    "The steepest descent is determined by the gradient, which is the vector of partial derivatives of the cost function with respect to each model parameter. \n",
    "The gradient indicates the direction of the fastest increase in the cost function, so the opposite direction of the gradient is the direction of the fastest decrease in the cost function.\n",
    "\n",
    "Gradient descent is used in machine learning to train models such as linear regression, logistic regression, and neural networks.\n",
    "The cost function of these models is typically non-convex, meaning it has multiple local minima. \n",
    "Gradient descent can get stuck in a local minimum, which may not be the global minimum.\n",
    "To overcome this problem, variants of gradient descent such as momentum, Adagrad, and Adam have been developed to improve the convergence rate and avoid local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5d72f-1598-4dd9-8a28-ced12d36fac2",
   "metadata": {},
   "source": [
    "# q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c57e2e-6068-41de-9edf-bc0acf43f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The multiple linear regression model is an extension of the simple linear regression model that allows for the analysis of the relationship between a dependent variable and two or more independent variables. \n",
    "In multiple linear regression, the dependent variable is modeled as a linear combination of the independent variables, with an added intercept term:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βpXp + ε\n",
    "\n",
    "where Y is the dependent variable, β0 is the intercept term, β1, β2, ..., βp are the coefficients or slopes of the independent variables X1, X2, ..., Xp, respectively, and ε is the random error term.\n",
    "\n",
    "Multiple linear regression differs from simple linear regression in that it allows for the analysis of the relationship between a dependent variable and multiple independent variables. \n",
    "Simple linear regression, on the other hand, only allows for the analysis of the relationship between a dependent variable and a single independent variable.\n",
    "\n",
    " multiple linear regression is a more complex model than simple linear regression that allows for the analysis of the relationship between a dependent variable and multiple independent variables. It allows us to estimate the individual effect of each independent variable on the dependent variable, while holding all other independent variables constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87274d-689e-4df3-a89f-fdb5431b63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331142e-a513-4092-a05b-93f9ac420bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in multiple linear regression refers to the situation where two or more independent variables are highly correlated with each other.\n",
    "This can cause problems in the regression analysis because it becomes difficult to determine the individual effect of each independent variable on the dependent variable. \n",
    "Multicollinearity can also lead to unstable and unreliable estimates of the regression coefficients.\n",
    "\n",
    "Multicollinearity can be detected by examining the correlation matrix of the independent variables. \n",
    "If the correlation coefficient between two or more independent variables is greater than 0.7 or 0.8, then multicollinearity may be present.\n",
    "\n",
    "Remove one of the highly correlated independent variables from the analysis. This can be done by examining the importance of each independent variable in the model and selecting the most important ones.\n",
    "\n",
    "Combine the highly correlated independent variables into a single variable. This can be done by using principal component analysis or factor analysis to create a new variable that represents the common variance between the highly correlated variables.\n",
    "\n",
    "Use regularization techniques such as Ridge regression or Lasso regression. These techniques add a penalty term to the regression equation that shrinks the regression coefficients towards zero, thereby reducing the impact of multicollinearity.\n",
    "\n",
    "Increase the sample size. Multicollinearity is less of an issue when the sample size is large because there is more variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce44b496-0608-4a77-908e-9adb8537fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495eef8-fd6f-414f-a347-a072ce6ca5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable X and the dependent variable Y is modeled as an nth degree polynomial.\n",
    "This is in contrast to linear regression, which models the relationship as a straight line.\n",
    "\n",
    "The polynomial regression model can be written as:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + β3X^3 + ... + βnX^n + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, β0, β1, β2, ..., βn are the regression coefficients or slopes, X^2, X^3, ..., X^n are the higher order terms, and ε is the error term.\n",
    "\n",
    "The polynomial regression model allows for a more flexible relationship between X and Y compared to linear regression, which assumes a constant slope.\n",
    "By including higher order terms, the polynomial regression model can capture non-linear relationships between the variables.\n",
    "However, this increased flexibility comes at the cost of increased complexity and a potential for overfitting the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944ecc55-3601-4f77-80e7-f6f826f00de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c455b5-02f7-45cb-bb96-6df0c873bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advantages\n",
    "1.Flexibility: Polynomial regression can capture non-linear relationships between the independent and dependent variables, whereas linear regression only models linear relationships.\n",
    "\n",
    "2.Better fit: In situations where the relationship between the independent and dependent variables is non-linear, a polynomial regression model may fit the data better than a linear regression model.\n",
    "\n",
    "# disadvantages\n",
    "1.Overfitting: Polynomial regression can be prone to overfitting, where the model fits the training data too closely and fails to generalize to new data.\n",
    "\n",
    "2.Complexity: As the degree of the polynomial increases, the model becomes more complex and more difficult to interpret.\n",
    "\n",
    "3.Extrapolation: Extrapolation with a polynomial regression model can be unreliable as the model may produce unrealistic predictions outside of the range of the training data.\n",
    "\n",
    "In situations where the relationship between the independent and dependent variables is non-linear, polynomial regression may be preferred over linear regression.\n",
    "For example, in a scenario where the amount of rainfall affects crop yield, a polynomial regression model could capture the diminishing returns of yield as rainfall increases. \n",
    "However, if the relationship between the variables is linear or close to linear, linear regression may be more appropriate as it is simpler and less prone to overfitting.\n",
    "\n",
    "Ultimately, the choice between polynomial regression and linear regression depends on the nature of the data and the goal of the analysis. \\\n",
    "It is important to assess the fit and complexity of both models and select the one that best suits the needs of the analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
